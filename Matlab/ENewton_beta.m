function [Y, infos] = ENewton_beta(D, pars)
%
% call: [Y, infos] = ENewton_beta(D, []) to use default parameters
% call: [Y, infos] = ENewton_beta(D, pars) to provide some parameters
%
% Input: D  -- Squared predistance matrix (diag(D) =0)
%       
% pars: -- parameters needed as input
%
% pars.y -- starting point, a vector of length(D)
%              default: y = zeors(n,1);
% pars.eig -- eigensolver
%             1 (for eig.m, default)
%             0 (for mexeig.m provided by defeng)
% pars.eigenlevel -- 1.0e-5 (yes, treat >= 1.0e-5 as positive eigenvalues)
% pars.scaling -- = 1 no scaling 
%                 = max(max(D))  (default)
% pars.tol -- error tolerance (default: 1.0e-6)
% pars.printyes -- printyes = 1 print information
%                             0 no output information (default)
% Output:
%      Y -- Nearest Euclidean distance matrix from D
%    infos:
%      infos.X  = Embedding coordinates generated by Y
%                 -0.5JYJ = X^TX, and X =[x1, x2, ..., xn]
%      infos.y  = the optimal Lagrange multiplier corresponding 
%                 to the diagonal constraints
%      infos.Z  = Z the complementarity matrix
%                 Y - D + diag(y) - Z = 0
%                 and Y is the output (trace(YZ) = 0 in theory)
%      infos.lambda: positive eiegnvalues of (-0.5JYJ) (decreasing order)
%      infos.P: leading eigenvectors of (-0.5JYJ) (corresponding to lambda)
%      infos.Pold: (eigenvectors of (-J(-D+\A^*(y))J)
%      infos.lambdaold: eigenvalues of (-J(-D+\A^*(y))J) in decreasing order
%      infos.rank = Embedding_Dimension;
%      infos.Iter = k;
%      infos.feval = f_eval; number of function evaluations;
%      infos.t = time_used; total cpu used
%      infos.res = norm_b; norm of the gradient at the final iterate
%                  This is the residual one finally got.
%      infos.f = val_obj; final objective function value 
%      infos.EigenD -- Total number of eigen decompositions
%%
%  This code is designed to solve %%%%%%%%%%%%%
%   min 0.5*<X-D, X-D>
%   s.t. X_ii =0, i=1,2,...,n
%        (-X) is positive semidefinite on the subspace
%          \{x\in \Re^n: eTx =0 \}
% In this version: D is first scaled to D/max(max(D)) to bring
%                  all distances between 0 and 1.
% In the outout: Information is scaled back. The intermidate output 
%                information is for the scaled problem.
%%
%  Based on the algorithm  in %%%%%
%  ``A Semismooth Newton Method for 
%  the Nearest Euclidean Distance Matrix Problem'' 
%  (SIMAX 34(1), 2013, pp. 67--93)
%                By 
%             Houduo Qi                        
%   
%  First version  date:  August 16, 2011
%  Second version date:  June 19,   2012  
%  Current Version date: March 8,   2013
%%          
% Send your comments and suggestions to    %%%%%%
%        hdqi@soton.ac.uk      %%%%%%
% changes by Qingna Li: 
%  1. Myeig function for eigenvalue decomposition
%  2. simplify a little bit of grad function
%  3. Change JyJ: 
% 
% Acknowledgment: The code makes use some technieques developed by
% Houduo Qi and Defeng Sun (NUS) for computing 
% the nearest correlation matrix  (CorNewton.m)
%
%%%%% Warning: Accuracy may not be guaranteed!!!!! %%%%%%%%
%%
%
t0 = tic;
n = length(D);
% scale D so that D_{ij} \in [0,1]
Sfactor = abs(max(max(D))); % Sfactor = 1 to return to unscaled version
%%%%%
% set up parameters
%
if ~isfield(pars, 'scaling')
    pars.scaling = Sfactor;
end
if ~isfield(pars, 'y')
    pars.y = zeros(n,1);
end
if ~isfield(pars, 'eig')
    pars.eig = 1;
end
if ~isfield(pars, 'eigenlevel')
    pars.eigenlevel = 0;
    pars.eigenyes = 0;
else
    pars.eigenyes = 1;
end
if ~isfield(pars, 'tol')
    pars.tol = 1.0e-6;
end
if ~isfield(pars, 'printyes')
    pars.printyes = 0;
end
%
Sfactor = pars.scaling;
D    = D/Sfactor;
prnt = pars.printyes;
y    = pars.y/Sfactor;
error_tol = pars.tol/Sfactor;
if error_tol < 1.0e-9
   error_tol = 1.0e-7;
end
eigsolver = pars.eig;
positive_eigenlevel = pars.eigenlevel;
positive_eigenyes = pars.eigenyes;
%
if prnt
   fprintf('\n ******************************************************** \n')
   fprintf( '          The Semismooth Newton-CG Method (ENewton.m)        ')
   fprintf('\n ******************************************************** \n')
   fprintf('\n The information of this problem is as follows: \n')
   fprintf(' Dim. of    sdp      constr  = %d \n',n)
end

D = -(D+D')/2; % make D symmetric
               % use -D instead because it is (-D) which is psd on e^\perp
%
% calculate JDJ
%
De  = sum(D, 2); % row sums
eDe = sum(De); % total sum of D
JDJ = -De*(ones(1,n)/n);
JDJ = JDJ + JDJ';
JDJ = JDJ + D + eDe/n^2;
%JDJ = (JDJ + JDJ')/2; % make JDJ symmetric
%
k      = 0;
f_eval = 0;
EigenD      = 0; % eigen decomposition number
Iter_Whole  = 200;
Iter_inner  = 20; % Maximum number of Line Search in Newton method
maxit = 200; %Maximum number of iterations in PCG
iterk = 0;
Inner = 0;
tol   = 1.0e-2; %relative accuracy for CGs
%
sigma_1=1.0e-4; %tolerance in the line search of the Newton method
%
x0=y;
%
prec_time = 0;
pcg_time = 0;
eig_time =0;

c = ones(n,1);
%M = diag(c); % Preconditioner to be updated
%
val_G = sum(sum(D.*D))/2;
%
% calculate Y = - J(D+diag(y))J
%
Y = JyJ(y);
Y = - (JDJ + Y);
Y = (Y+Y')/2;
%%%%% eigendecomposition
eig_time0 = tic;
%%% there are two ways to do this
%%% one is to use matlab: eig
%%% the other is to use mexeig (64-bit) written by Defeng
%%% use matlab eig.m
 [P,lambda] = MYeig(Y,eigsolver); 
 eig_time = eig_time + toc(eig_time0); 
 EigenD   = EigenD + 1;
 
if (positive_eigenyes)
   lambdan = max(-lambda, 0);
   lambda(lambda < positive_eigenlevel) = 0;
   lambda = lambda - lambdan;
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 [f0,Fy] = gradient(y,D,lambda,P,n);
 f = f0;
 f_eval = f_eval + 1; % number of function evaluations increased by 1
 b = - Fy;
 norm_b = norm(b);

 Initial_f = val_G - f0;
if prnt
    fprintf('Initial Dual Objective Function value==== %d \n', Initial_f)
    fprintf('Newton: Norm of Gradient %d \n',norm_b)
end

 Omega12 = omega_mat(lambda,n);
 x0 = y;

 tt = toc(t0);
[hh,mm,ss] = time(tt);

if prnt
    fprintf('\n   Iter.   Num. of CGs     Step length      Norm of gradient     time_used ')
    fprintf('\n    %d         %2.0d            %3.2e                      %3.2e         %d:%d:%d ',0,str2num('-'),str2num('-'),norm_b,hh,mm,ss)
end

 while (norm_b>error_tol & k < Iter_Whole)

    prec_time0 = tic;
    c = precond_matrix(Omega12,P,n); % comment this line for  no preconditioning
    prec_time = prec_time + toc(prec_time0);
    pcg_time0 = tic;
    [d,flag,~,iterk]  =pre_cg(b,tol,maxit,c,Omega12,P,n);
    pcg_time = pcg_time + toc(pcg_time0);
 %d =b0-Fy; gradient direction
 %fprintf('Newton: Number of CG Iterations %d \n', iterk)
  
    if (flag~=0); % if CG is unsuccessful, use the negative gradient direction
     % d =b0-Fy;
         disp('..... Not a full Newton step......')
    end
    slope = (Fy)'*d; %%% nabla f d 

    y = x0 + d; %temporary x0+d  
    Y = JyJ(y);
    Y = - (JDJ + Y);
    Y = (Y+Y')/2;
%%%%% eigendecomposition
    eig_time0 = tic;
    [P,lambda] = MYeig(Y,eigsolver); 
    eig_time = eig_time + toc(eig_time0); 
    EigenD   = EigenD   + 1;

    if (positive_eigenyes)
        lambdan = max(-lambda, 0);
        lambda(lambda < positive_eigenlevel) = 0;
        lambda = lambda - lambdan;
    end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    [f,Fy] = gradient(y,D,lambda,P,n); % increase of f_eval will be added
                            % after the linear search  
    k_inner = 0;
    while(k_inner <=Iter_inner & f> f0 + sigma_1*0.5^k_inner*slope + 1.0e-6)
        % line search procedure
        k_inner = k_inner+1;
        y = x0 + 0.5^k_inner*d; % backtracking   

        Y = JyJ(y);
        Y = - (JDJ + Y);
        Y = (Y+Y')/2;

        %%%%% eigdecomposition
        %%%%% eigendecomposition         
        eig_time0 = tic;
        [P,lambda] = MYeig(Y,eigsolver); 
        eig_time = eig_time + toc(eig_time0);
        EigenD   = EigenD   + 1;
        %%% end of eigendecompsition

        if (positive_eigenyes)
            lambdan = max(-lambda, 0);
            lambda(lambda < positive_eigenlevel) = 0;
            lambda = lambda - lambdan;
        end

        [f,Fy] = gradient(y,D,lambda,P,n);
    end % end of the line search procedure
    if k_inner >=1
        fprintf('\n number of linear serach: %d', k_inner)
    end
    f_eval = f_eval + k_inner + 1; % number of function evaluations
                         % is increased over the line search
                         % and the one before it
    x0 = y;
    f0 = f;

    k=k+1;
    b = -Fy;
    norm_b = norm(b);
    tt = toc(t0);
    [hh,mm,ss] = time(tt); 
    %   fprintf('Newton: Norm of Gradient %d \n',norm_b)
    if prnt      
        fprintf('\n   %2.0d         %2.0d             %3.2e          %3.2e         %d:%d:%d', ...
        k,iterk,0.5^k_inner,norm_b,hh,mm,ss)
    end
    Res_b(k) = norm_b;    
    Omega12 = omega_mat(lambda,n);
 end
 %end loop for while i=1;

Ip = find(lambda>0); % could set to 1.0e-7
%In = find(lambda<0); % could set to -1.0e-7
Embedding_Dimension = sum(lambda < - eps^(3/4) ); % that is the length of In
  % The eigen-decomposition is on -J(D+\A^*y)J
  % The imbedding dimension is the number of negative eigenvalues of
  % -J(D+\A^*y)J.
  % This result is based on Haydan-Wells projection formula
  % see Eq. (37) in my paper.
r = length(Ip);
 
if (r==0)
    Y = D + diag(y);
elseif (r==n)
    Y = D + diag(y) + Y;
elseif (r<=n/2)
    lambda1 = lambda(Ip);
    lambda1 = lambda1.^0.5;
    P1 = P(:, 1:r);
    P1 = P1*sparse(diag(lambda1));
    Y = P1*P1';
    Y = D+diag(y) + Y;% Optimal solution X* 
else     
    lambda2 = -lambda(r+1:n);
    lambda2 = lambda2.^0.5;
    P2 = P(:, r+1:n);
    P2 = P2*sparse(diag(lambda2));
    Y = Y + P2*P2'; 
    Y = D+diag(y) + Y;% Optimal solution X* 
end
 Y = (Y+Y')/2;
 Final_f = Sfactor^2*(val_G-f);
 %
 % set diagonals of Y to zero
 %
 Y(1:(n+1):end) = 0;
 %
 % dual matrix Z
 Z = - D + Y - diag(y); % -D = Dold/scaleD;
 Z = -Z; % Z = -Y + D + diag(y)
% comp_gap = sum(sum(Y.*Z)); % complementarity gap trace(YZ) = 0 in theory
 
 val_obj   = sum(sum((Y-D).*(Y-D)))/2;
 val_obj   = val_obj*Sfactor^2;
 norm_b    = norm_b*Sfactor;
 y         = Sfactor*y;
 lambda    = Sfactor*lambda;
 time_used = toc(t0); 
% fprintf('\n')
% set output information
infos.Pold      = P;
infos.lambdaold = lambda;
infos.rank   = Embedding_Dimension;
infos.Iter   = k;
infos.feval  = f_eval;
infos.t      = time_used;
infos.res    = norm_b;
infos.f      = val_obj;
infos.Z  = Sfactor*Z; % optimal dual solution in \K^*
infos.y  = y;
infos.EigenD = EigenD;
%
% Put the sign (-1) back
%
 Y = -Sfactor*Y;
 Y = abs(Y);
%
% check the complementarity condition:
% Y- Dold + diag(y) - Z = 0 % here Dold = -D*Sfactor
%  Z = Y + Sfactor*D + diag(y); % = infos.Z
%  comp_gap = sum(sum(Y.*Z));

% Embedding coordinates generated by the calculated nearest EDM: Y
% - 0.5JYJ = 0.5 * Sfactor * \Pi_{\S^n_+} (J(-D + \A^*(y))J) = X^TX
% Note: It is those negative eigenvalues in lambda that gives X^TX
%
lambda = abs(lambda); % lambda has been mulitplied by Sfactor
lambda = lambda(end:-1:1);
P = P(:, end:-1:1);
infos.P = P;
infos.lambda = lambda;

lambda = 0.5*lambda(1:Embedding_Dimension); % positive eigenvalues in -0.5JYJ
P = P(:, 1:Embedding_Dimension);
X = P*diag(lambda.^(0.5));
X = X';
%

infos.X = X;
infos.dualVal   = Final_f;
infos.primVal   = val_obj;
infos.res = norm_b;
infos.r = Embedding_Dimension;
infos.time = time_used;
infos.y = y;

if prnt
    fprintf('\n\n')
    fprintf('Norm of Gradient == %d \n', full(norm_b))
    fprintf('Number of Iterations == %d \n', k)
    fprintf('Number of Function Evaluations == %d \n', f_eval)
    fprintf('Final Dual Objective Function value ========== %d \n', full(Final_f))
    fprintf('Final Original Objective Function value ====== %d \n', full(val_obj))
    fprintf('Embedding dimension ================= %d \n',Embedding_Dimension)

    fprintf('Computing time for computing preconditioners == %d \n', prec_time)
    fprintf('Computing time for linear systems solving (cgs time) ====%d \n', pcg_time)
    if eigsolver
       fprintf('Computing time for  eigenvalue decompostions (calling eig time)==%d \n', eig_time)
    else
       fprintf('Computing time for  eigenvalue decompostions (calling mexeig time)==%d \n', eig_time) 
    end
    fprintf('Total computing time (in s) ==== =====================%d \n',time_used)
end
% 


%%% end of the main program

%%% To change the format of time 
function [h,m,s] = time(t)
t = round(t); 
h = floor(t/3600);
m = floor(rem(t,3600)/60);
s = rem(rem(t,60),60);
return
%%% End of time.m

%%%%%% To generate J*diag(y)J
%
function Y = JyJ(y)

n = length(y);
y1 = -y/n;
Y = y1(:,ones(1,n));
% Y = - y*(ones(1,n)/n); original calculation
Y = Y+Y';
Y = Y + diag(y) + sum(y)/n^2;

return
%% end of JyJ

%%% mexeig decomposition
function [P,lambda] = MYeig(X,eigsolver)
if eigsolver %% use matlab built-in function eig
     [P, Lambda] =  eig(X);   %% X= P*diag(D)*P'
     P = real(P);
     lambda = real(diag(Lambda));
    
else %% use mexeig developed by Prof. Defeng Sun
    [P,lambda] = mexeig(X);
    P          = real(P);
    lambda     = real(lambda);    
    
end

%% make sure eigenvalues are in decreasing order
if issorted(lambda)
    lambda = lambda(end:-1:1);
    P      = P(:,end:-1:1);
elseif issorted(lambda(end:-1:1))
    return;
else
    [lambda, Inx] = sort(lambda,'descend');
    P = P(:,Inx);
end
return
%%% End of MYeig.m

%%%%%%
%%%%%% To generate F(y) %%%%%%%
%%%%%%%

function [f,Fy]= gradient(y,D,lambda,P,n)
 
%[n,n]=size(P);
 f = 0.0;
Fy = zeros(n,1);
 
%lambdap=max(0,lambda);
%H =diag(lambdap); %% H =P^T* H^0.5*H^0.5 *P

r = sum(lambda>0);
lambda1 = lambda(1:r).^0.5;
P= P(:,1:r)';
for i = 1:r;
    P(i,:) = lambda1(i)*P(i,:);
end
 
 i=1;
 while (i<=n)
       Fy(i) = P(:,i)'*P(:,i) + y(i) + D(i,i);
       i=i+1;     
 end
  
 Dy = D + diag(y);
 
 % Compute the objective function (because it is easy to calculate)
%
r = sum(lambda<0); % negative eigenvalues
lambdan = lambda(end:-1:(n-r+1));
f = f + lambdan'*lambdan;
%
% the rest part depends on Q
%
Dye = sum(Dy, 2); % Dy*e
%
% to save using a new variable yhat (see below)
%
c = (sum(Dye) + sqrt(n)*Dye(n))/(n+sqrt(n));
Dye = (-Dye + c)/sqrt(n); %  = -Dye/sqrt(n) + c/sqrt(n);
                          % denoted by Dye (Dye not to be used any more)
Dye(end) = Dye(end) + c;
 
 f = f + sum(Dye.^2) + sum(Dye(1:end-1).^2);
 f = 0.5*f;

return

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% end of gradient.m %%%%%%

%%%%%%%%%%%%%%        %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% To generate the first -order difference of lambda
%%%%%%%

%%%%%%%%%%%%%%        %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% To generate the essential part of the first -order difference of d
%%%%%%%
function Omega12 = omega_mat(lambda,n)
%We compute omega only for 1<=|idx|<=n-1
idx.idp = find(lambda>0);
idx.idm = setdiff([1:n],idx.idp);
n =length(lambda);
r = length(idx.idp);
 
if ~isempty(idx.idp)
    if (r == n)
        Omega12 = ones(n,n);
    else
        s = n-r;
        dp = lambda(1:r);
        dn = lambda(r+1:n);
        Omega12 = (dp*ones(1,s))./(abs(dp)*ones(1,s) + ones(r,1)*abs(dn'));
        %  Omega12 = max(1e-15,Omega12);

    end
else
    Omega12 =[];
end

    %%***** perturbation *****
    return

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% end of omega_mat.m %%%%%%%%%%

%%%%%% PCG method %%%%%%%
%%%%%%% This is exactly the algorithm by  Hestenes and Stiefel (1952)
%%%%%An iterative method to solve A(x) =b  
%%%%%The symmetric positive definite matrix M is a
%%%%%%%%% preconditioner for A. 
%%%%%%  See Pages 527 and 534 of Golub and va Loan (1996)

function [p,flag,relres,iterk] = pre_cg(b,tol,maxit,c,Omega12,P,n);
% Initializations
r = b;  %We take the initial guess x0=0 to save time in calculating A(x0) 
n2b =norm(b);    % norm of b
tolb = tol * n2b;  % relative tolerance 
p = zeros(n,1);
flag=1;
iterk =0;
relres=1000; %%% To give a big value on relres
% Precondition 
z =r./c;  %%%%% z = M\r; here M =diag(c); if M is not the identity matrix 
rz1 = r'*z; 
rz2 = 1; 
d = z;
% CG iteration
for k = 1:maxit
   if k > 1
       beta = rz1/rz2;
       d = z + beta*d;
   end
   %w= Jacobian_matrix(d,Omega,P,n); %w = A(d); 
   w = Jacobian_matrix(d,Omega12,P,n); % W =A(d)
   denom = d'*w;
   iterk =k;
   relres = norm(r)/n2b;              %relative residue = norm(r) / norm(b)
   if denom <= 0 
       sssss=0
       p = d/norm(d); % d is not a descent direction
       break % exit
   else
       alpha = rz1/denom;
       p = p + alpha*d;
       r = r - alpha*w;
   end
   z = r./c; %  z = M\r; here M =diag(c); if M is not the identity matrix ;
   if norm(r) <= tolb % Exit if Hp=b solved within the relative tolerance
       iterk =k;
       relres = norm(r)/n2b;          %relative residue =norm(r) / norm(b)
       flag =0;
       break
   end
   rz2 = rz1;
   rz1 = r'*z;
end

return

%%%%%%%% %%%%%%%%%%%%%%%
%%% end of pre_cg.m%%%%%%%%%%%


%%% To generate the Jacobian product with x: F'(y)(x)
function Ax = Jacobian_matrix(x,Omega12,P,n)

[r,s]  = size(Omega12); 
Ax = zeros(n,1);
tau = 1.0e-10;

if (r==0)
    Ax = (1 + tau)*x;
    return
elseif (r==n)
    Ax = (2/n+tau)*x - sum(x)/n^2;
    return
end

P1 = P(:,1:r);
P2 = P(:,r+1:n);
    
%PT = P'; % PT can be saved to save memory
pbar = sum(P)'/n; %sum(P', 2)/n;
sumx = sum(x);
startx = P'*x;
startx = 0.5*sumx*pbar - startx;

PX = P'.*startx(:, ones(n,1));
PY = P'.*pbar(:, ones(n,1));
    
if (r<n/2)
   H1 = P1'*sparse(diag(x));
   Omega12_old = Omega12;
   Omega12 = Omega12.*(H1*P2);
   H = [(H1*P1)*P1' + Omega12*P2'; Omega12'*P1'];

   PX12 = Omega12_old'*PX(1:r, :);
   PY12 = PY(1:r, :)'*Omega12_old;
%%%%% 1st way to calculate Ax           
%             PY12 = PY12'.*PX(r+1:n, :);
%             PX12 = PY(r+1:n, :).*PX12;
%             
%              Ax = sum(PY(1:r, :)).*sum(PX(1:r, :)) + sum(PX12+PY12);
%              Ax = 2*Ax + sum(P'.*H);
%              Ax = Ax';
%              Ax = (1+1.0e-10)*x - Ax;
%%%%%%
%%%%%% 2nd way to calculate Ax
    i=1;
    while (i<=n)
        Ax(i) = P(i,:)*H(:,i); % part from the digonal part
        v  = sum(PY(1:r, i))*sum(PX(1:r, i)) ...
           + PY12(i, :)*PX(r+1:n, i) + PY(r+1:n, i)'*PX12(:, i);
        Ax(i) = x(i) - Ax(i) - 2*v;  
        i=i+1;
    end
    Ax = Ax + 1.0e-10*x; 
%%%%%%%%%%%%% end of 2nd way
else % if r>=n/2, use a complementary formula.
     %H = ((E-Omega).*(P'*Z*P))*P';               
     H2 = P2'*sparse(diag(x));
     Omega12 = ones(r,s)- Omega12;
     Omega12_old = Omega12;
     Omega12 = Omega12.*((H2*P1)');
     H = [Omega12*P2'; Omega12'*P1' + (H2*P2)*P2'];
           
     PX12 = Omega12_old*PX(r+1:n, :);
     PY12 = PY(r+1:n, :)'*(Omega12_old');
%%%%% 1st way to calculate Ax            
%             PX12 = PY(1:r, :).*PX12;
%             PY12 = PY12'.*PX(1:r, :);
%             
%             Ax = sum(PY(r+1:n, :)).*sum(PX(r+1:n, :)) + sum(PX12+PY12);
%             Ax = 2*Ax + sum(P'.*H);
%             Ax = Ax';
%%%%% 2nd way to calculate Ax 
    i=1;
    while (i<=n)
       Ax(i) = P(i,:)*H(:,i);
       v  = sum(PY(r+1:n, i))*sum(PX(r+1:n, i)) ...
          + PY(1:r, i)'*PX12(:, i) + PY12(i, :)*PX(1:r, i);
       Ax(i) = Ax(i) + 2*v;  
       i=i+1;   
    end
%%%%%% end of 2nd way
     Ax = (2/n+1.0e-10)*x - sumx/n^2 + Ax;
end
return
%%% End of Jacobian_matrix.m  
   

%%%%%% To generate the diagonal preconditioner%%%%%%%
%%%%%%%

function c = precond_matrix(Omega12,P,n)

r =size(Omega12, 1);
c = ones(n,1);

if (r==0) % Omega = 0
    return % c = ones(n,1);
elseif (r==n) % Omega = E
    t = 1/n;
    c = (2*t - t.^2)*ones(n,1);
    return
end
%
H = P';
h = sum(H,2)/n; %average row sum
H = H.*(H-h(:, ones(n,1))); % H.*(H-[h,h,...,h])

if (r>0) && (r< n/2)
   H12 = H(1:r,:)'*Omega12;
   for i=1:n
       c(i) = (sum(H(1:r,i))).^2;
       c(i) = c(i) +2.0*(H12(i,:)*H(r+1:n,i));
       c(i) = 1 - c(i);
   end
else % r>=n/2, use a complementary formula
    Omega12 = 1 - Omega12;
    H12 = Omega12*H(r+1:n,:);
    for i=1:n
        c(i) = (sum(H(r+1:n,i)))^2;
        c(i) = c(i) + 2.0*(H(1:r,i)'*H12(:,i));
%         alpha = sum(H(:,i));
%         c(i) = alpha^2-c(i);
%         c(i) = 1- c(i);
    end
end
c = max(1.0e-8, c);

return

 
%%%%%%%%%%%%%%%
%end of precond_matrix.m%%%